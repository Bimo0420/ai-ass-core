# GPU-ускоренная версия Ollama и LlamaIndex
version: '3.8'

services:
  # Ollama с поддержкой GPU
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    runtime: nvidia
    restart: unless-stopped
    networks:
      - ai-network
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama.rule=Host(`ollama.${DOMAIN}`)"
      - "traefik.http.routers.ollama.entrypoints=websecure"
      - "traefik.http.routers.ollama.tls.certresolver=letsencrypt"
      - "traefik.http.services.ollama.loadbalancer.server.port=11434"

  # LlamaIndex RAG API (CPU версия для стабильности)
  llamaindex:
    image: llamaindex-api:latest
    container_name: llamaindex
    restart: unless-stopped
    networks:
      - ai-network
    ports:
      - "8000:8000"
    environment:
      OLLAMA_URL: ${OLLAMA_URL}
      SUPABASE_URL: http://supabase-kong:8000
      SUPABASE_SERVICE_KEY: ${SERVICE_ROLE_KEY}
      POSTGRES_URL: postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./llamaindex/data:/app/data
      - huggingface-cache:/root/.cache/huggingface
    depends_on:
      - ollama
      - postgres

volumes:
  ollama-data:
  huggingface-cache:

