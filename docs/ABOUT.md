# 🧠 Как работает AI Stack — Подробный Workflow

Этот документ описывает, что происходит "под капотом" при каждом запросе пользователя через интерфейс Open WebUI.

---

## 📊 Общая схема взаимодействия

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              ПОЛЬЗОВАТЕЛЬ                                    │
│                         (Браузер / API клиент)                               │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                              TRAEFIK                                         │
│                    (Reverse Proxy + SSL Termination)                         │
│                                                                              │
│  • Принимает HTTPS запрос на openwebui.your-domain.com                      │
│  • Проверяет SSL сертификат (Let's Encrypt)                                 │
│  • Маршрутизирует запрос к нужному контейнеру                               │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                            OPEN WEBUI                                        │
│                        (Веб-интерфейс чата)                                  │
│                                                                              │
│  • Аутентификация пользователя                                              │
│  • Формирование запроса к LLM                                               │
│  • Стриминг ответа в UI                                                     │
│  • Сохранение истории чатов                                                 │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                    ┌───────────────┼───────────────┐
                    │               │               │
                    ▼               ▼               ▼
            ┌───────────┐   ┌───────────┐   ┌───────────────┐
            │  OLLAMA   │   │ PIPELINES │   │   LANGFUSE    │
            │   (LLM)   │   │  (Tools)  │   │  (Tracing)    │
            └───────────┘   └───────────┘   └───────────────┘
```

---

## 🔄 Детальный Workflow: Простой чат-запрос

### Шаг 1: Пользователь отправляет сообщение

```
Пользователь: "Расскажи про нейронные сети"
```

**Что происходит:**

1. Браузер отправляет HTTPS POST запрос на `https://openwebui.your-domain.com/api/chat/completions`
2. Запрос содержит:
   - Текст сообщения
   - ID выбранной модели (например, `llama3.2`)
   - История предыдущих сообщений (контекст)
   - Настройки (temperature, max_tokens и т.д.)

---

### Шаг 2: Traefik обрабатывает запрос

```
┌─────────────────────────────────────────┐
│              TRAEFIK                     │
├─────────────────────────────────────────┤
│ 1. Принимает запрос на порт 443         │
│ 2. Проверяет SSL сертификат             │
│ 3. Парсит Host header                   │
│ 4. Находит правило маршрутизации:       │
│    Host(`openwebui.${DOMAIN}`)          │
│ 5. Проксирует на openwebui:8080         │
└─────────────────────────────────────────┘
```

**Конфигурация в docker-compose:**
```yaml
labels:
  - "traefik.http.routers.openwebui.rule=Host(`openwebui.${DOMAIN}`)"
  - "traefik.http.services.openwebui.loadbalancer.server.port=8080"
```

---

### Шаг 3: Open WebUI обрабатывает запрос

```
┌─────────────────────────────────────────┐
│            OPEN WEBUI                    │
├─────────────────────────────────────────┤
│ 1. Проверяет аутентификацию (JWT токен) │
│ 2. Загружает историю чата из SQLite     │
│ 3. Формирует промпт с системным         │
│    сообщением + история + новый запрос  │
│ 4. Определяет endpoint: Ollama          │
│ 5. Отправляет запрос на Ollama API      │
└─────────────────────────────────────────┘
```

**Переменные окружения:**
```yaml
environment:
  - OLLAMA_BASE_URL=${OLLAMA_URL}  # http://ollama:11434
  - LANGFUSE_HOST=http://langfuse:3000
  - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
```

---

### Шаг 4: Ollama генерирует ответ

```
┌─────────────────────────────────────────┐
│              OLLAMA                      │
├─────────────────────────────────────────┤
│ 1. Получает запрос от Open WebUI        │
│ 2. Загружает модель в GPU/CPU память    │
│    (если не загружена)                  │
│ 3. Токенизирует входной текст           │
│ 4. Запускает инференс модели            │
│ 5. Декодирует токены в текст            │
│ 6. Стримит ответ по частям              │
└─────────────────────────────────────────┘
```

**Процесс инференса:**
```
Входной текст → Токенизация → [Token IDs] → 
→ Модель (Forward Pass) → [Logits] →
→ Sampling (temperature) → [Next Token] →
→ Декодирование → Текст ответа
```

**Настройки Ollama:**
```yaml
environment:
  - OLLAMA_KEEP_ALIVE=24h        # Держать модель в памяти
  - OLLAMA_MAX_LOADED_MODELS=2   # Макс. моделей в памяти
```

---

### Шаг 5: Langfuse логирует запрос (параллельно)

```
┌─────────────────────────────────────────┐
│             LANGFUSE                     │
├─────────────────────────────────────────┤
│ 1. Получает trace от Open WebUI         │
│ 2. Записывает в ClickHouse:             │
│    - Промпт (вход)                      │
│    - Ответ (выход)                      │
│    - Время выполнения                   │
│    - Количество токенов                 │
│    - Метаданные (модель, user_id)       │
│ 3. Сохраняет большие данные в MinIO     │
└─────────────────────────────────────────┘
```

**Что сохраняется:**
```json
{
  "trace_id": "abc123",
  "timestamp": "2024-12-15T22:30:00Z",
  "model": "llama3.2",
  "input_tokens": 150,
  "output_tokens": 500,
  "latency_ms": 2340,
  "user_id": "user@example.com",
  "prompt": "Расскажи про нейронные сети",
  "response": "Нейронные сети — это..."
}
```

---

### Шаг 6: Ответ возвращается пользователю

```
Ollama → Open WebUI → Traefik → Браузер пользователя

Формат ответа (Server-Sent Events для стриминга):
data: {"message": {"content": "Нейронные"}}
data: {"message": {"content": " сети"}}
data: {"message": {"content": " —"}}
data: {"message": {"content": " это..."}}
data: [DONE]
```

---

## 🔄 Workflow: RAG-запрос (с использованием базы знаний)

Когда пользователь задает вопрос, требующий поиска в документах:

```
┌────────────────────────────────────────────────────────────────────────────┐
│                           RAG WORKFLOW                                      │
└────────────────────────────────────────────────────────────────────────────┘

     Пользователь
          │
          ▼
    ┌───────────┐
    │ Open WebUI│ ──────────────────────────────────────┐
    └─────┬─────┘                                       │
          │                                             │
          ▼                                             ▼
    ┌───────────┐                               ┌───────────────┐
    │    n8n    │ ◄──── Webhook trigger ────────│   Langfuse    │
    │ (Workflow)│                               │  (Tracing)    │
    └─────┬─────┘                               └───────────────┘
          │
          ├─────────────────┬─────────────────┐
          ▼                 ▼                 ▼
    ┌───────────┐     ┌───────────┐     ┌───────────┐
    │ Supabase  │     │LlamaIndex │     │  Ollama   │
    │(VectorDB) │     │ (RAG API) │     │(Embedding)│
    └───────────┘     └───────────┘     └───────────┘
          │                 │                 │
          └─────────────────┴─────────────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │    Ollama     │
                    │ (Generation) │
                    └───────────────┘
                            │
                            ▼
                      Финальный ответ
```

### Детальные шаги RAG:

#### 1. Запрос попадает в n8n через Webhook

```
Open WebUI → n8n Webhook
POST https://n8n.your-domain.com/webhook/rag-agent
{
  "message": "Какие марки бетона использовали в проекте?",
  "chat_id": "...",
  "user_id": "..."
}
```

#### 2. n8n запускает workflow

```
┌─────────────────────────────────────────────────────────────────┐
│                      n8n RAG Workflow                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐  │
│  │ Webhook  │ →  │ Postgres │ →  │  Ollama  │ →  │ AI Agent │  │
│  │ Trigger  │    │   Chat   │    │   Chat   │    │          │  │
│  │          │    │  Memory  │    │  Model   │    │          │  │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘  │
│                                                        │         │
│                                        ┌───────────────┘         │
│                                        ▼                         │
│                               ┌──────────────────┐              │
│                               │ Supabase Vector  │              │
│                               │     Store        │              │
│                               └──────────────────┘              │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### 3. Создание эмбеддинга запроса

```
┌─────────────────────────────────────────┐
│         OLLAMA (Embedding)               │
├─────────────────────────────────────────┤
│ Модель: nomic-embed-text                │
│                                         │
│ Вход: "Какие марки бетона..."           │
│       ↓                                 │
│ Выход: [0.123, -0.456, 0.789, ...]      │
│        (вектор размерности 768)         │
└─────────────────────────────────────────┘
```

#### 4. Поиск в векторной базе Supabase

```sql
-- Supabase Vector Search (pgvector)
SELECT 
  content,
  metadata,
  1 - (embedding <=> query_embedding) as similarity
FROM documents
ORDER BY embedding <=> query_embedding
LIMIT 5;
```

**Результат:**
```json
[
  {
    "content": "В проекте ЖК Северная Столица использовался бетон М400...",
    "similarity": 0.89,
    "source": "project_docs.pdf"
  },
  {
    "content": "Поставщик бетона: ООО СтройБетон, контракт от 15.03.2024...",
    "similarity": 0.85,
    "source": "contracts.xlsx"
  }
]
```

#### 5. Формирование контекста и генерация ответа

```
┌─────────────────────────────────────────┐
│         OLLAMA (Generation)              │
├─────────────────────────────────────────┤
│ Модель: llama3.2 / qwen2.5              │
│                                         │
│ Системный промпт:                       │
│ "Ты RAG-агент. Отвечай только на       │
│  основе предоставленного контекста..."  │
│                                         │
│ Контекст (из Supabase):                 │
│ "В проекте использовался бетон М400..." │
│                                         │
│ Вопрос пользователя:                    │
│ "Какие марки бетона использовали?"      │
│         ↓                               │
│ Ответ: "В проекте ЖК Северная Столица  │
│        использовался бетон марки М400   │
│        от поставщика ООО СтройБетон..." │
└─────────────────────────────────────────┘
```

---

## 🔄 Workflow: Автоматизация через n8n

### Пример: Автоматическая индексация документов

```
┌────────────────────────────────────────────────────────────────────────────┐
│                    DOCUMENT INDEXING WORKFLOW                               │
└────────────────────────────────────────────────────────────────────────────┘

  Загрузка файла         Парсинг          Чанкинг         Эмбеддинг
       │                    │                │                │
       ▼                    ▼                ▼                ▼
  ┌─────────┐         ┌─────────┐      ┌─────────┐      ┌─────────┐
  │   n8n   │   →     │ Docling │  →   │LlamaIdx │  →   │ Ollama  │
  │ Trigger │         │ (Parse) │      │ (Chunk) │      │(Embed)  │
  └─────────┘         └─────────┘      └─────────┘      └─────────┘
                                                              │
                                                              ▼
                                                        ┌─────────┐
                                                        │Supabase │
                                                        │(Store)  │
                                                        └─────────┘
```

**Детали:**

1. **Trigger (n8n)**: Отслеживает новые файлы или webhook
2. **Docling**: Парсит PDF, DOCX, XLSX в чистый текст
3. **LlamaIndex**: Разбивает текст на chunks (500-1000 токенов)
4. **Ollama**: Создает эмбеддинги для каждого chunk
5. **Supabase**: Сохраняет в PostgreSQL с pgvector

---

## 🗄️ Хранение данных

### PostgreSQL — Основная база

```
┌─────────────────────────────────────────────────────────────────┐
│                       POSTGRESQL                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Schema: public                                                  │
│  ├── n8n_*           → Данные n8n (workflows, credentials)      │
│  ├── documents       → Векторные эмбеддинги (pgvector)          │
│  └── chat_history    → История чатов для RAG                    │
│                                                                  │
│  Schema: langfuse                                                │
│  ├── traces          → Логи запросов к LLM                      │
│  ├── observations    → Детали каждого шага                      │
│  └── scores          → Оценки качества ответов                  │
│                                                                  │
│  Schema: auth (Supabase)                                        │
│  └── users           → Пользователи Supabase                    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### ClickHouse — Аналитика Langfuse

```
┌─────────────────────────────────────────┐
│            CLICKHOUSE                    │
├─────────────────────────────────────────┤
│ • Быстрая аналитика по логам           │
│ • Агрегация метрик токенов             │
│ • Хранение больших объемов traces      │
│ • Колоночное хранение для отчетов      │
└─────────────────────────────────────────┘
```

### MinIO — S3-совместимое хранилище

```
┌─────────────────────────────────────────┐
│              MINIO                       │
├─────────────────────────────────────────┤
│ Bucket: langfuse-events                 │
│ • Большие промпты/ответы               │
│ • Файлы трейсов                        │
│ • Вложения                             │
└─────────────────────────────────────────┘
```

### Redis — Кэш и очереди

```
┌─────────────────────────────────────────┐
│               REDIS                      │
├─────────────────────────────────────────┤
│ • Очередь задач n8n (Bull Queue)       │
│ • Кэш сессий Langfuse                  │
│ • Временное хранение состояния         │
└─────────────────────────────────────────┘
```

---

## ⚡ Производительность и масштабирование

### Узкие места и оптимизация

```
┌─────────────────────────────────────────────────────────────────┐
│                    BOTTLENECKS                                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. OLLAMA (LLM Inference)                                      │
│     └─→ Решение: GPU, OLLAMA_MAX_LOADED_MODELS=2                │
│                                                                  │
│  2. Vector Search (Supabase)                                    │
│     └─→ Решение: Индексы HNSW, лимит top_k                      │
│                                                                  │
│  3. n8n Queue                                                   │
│     └─→ Решение: n8n-worker для параллельной обработки          │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Архитектура масштабирования

```
                    ┌───────────────┐
                    │    Traefik    │
                    │ Load Balancer │
                    └───────┬───────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
   ┌─────────┐         ┌─────────┐         ┌─────────┐
   │  n8n    │         │  n8n    │         │  n8n    │
   │ Main    │         │ Worker1 │         │ Worker2 │
   └─────────┘         └─────────┘         └─────────┘
        │                   │                   │
        └───────────────────┼───────────────────┘
                            │
                    ┌───────▼───────┐
                    │    Redis      │
                    │   (Queue)     │
                    └───────────────┘
```

---

## 🔐 Безопасность в Workflow

### Аутентификация и авторизация

```
┌─────────────────────────────────────────────────────────────────┐
│                    SECURITY LAYER                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  TRAEFIK                                                        │
│  └── TLS 1.3, Let's Encrypt сертификаты                        │
│                                                                  │
│  OPEN WEBUI                                                     │
│  └── JWT токены, bcrypt хеширование паролей                    │
│                                                                  │
│  SUPABASE                                                       │
│  └── Row Level Security (RLS), JWT проверка                    │
│                                                                  │
│  N8N                                                            │
│  └── Encrypted credentials, API key authentication             │
│                                                                  │
│  LANGFUSE                                                       │
│  └── Project-level API keys, RBAC                              │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 📈 Мониторинг Workflow

### Что мониторится

```
┌─────────────────────────────────────────────────────────────────┐
│                    MONITORING STACK                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  PROMETHEUS (Метрики)                                           │
│  ├── n8n: выполнения workflows, ошибки, latency                │
│  ├── Node Exporter: CPU, RAM, Disk, Network                    │
│  └── Docker: container stats                                    │
│                                                                  │
│  GRAFANA (Визуализация)                                         │
│  ├── Dashboard: System Overview                                 │
│  ├── Dashboard: n8n Executions                                  │
│  └── Dashboard: LLM Performance                                 │
│                                                                  │
│  LANGFUSE (LLM Observability)                                   │
│  ├── Token usage per model                                      │
│  ├── Latency distribution                                       │
│  ├── Error rates                                                │
│  └── Cost estimation                                            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 🧪 Пример полного trace в Langfuse

```json
{
  "trace": {
    "id": "trace_abc123",
    "name": "RAG Query",
    "timestamp": "2024-12-15T22:30:00Z",
    "duration_ms": 3450,
    "user_id": "user@example.com",
    "metadata": {
      "chat_id": "chat_xyz",
      "model": "llama3.2"
    }
  },
  "observations": [
    {
      "name": "embedding",
      "type": "generation",
      "model": "nomic-embed-text",
      "input_tokens": 15,
      "duration_ms": 120
    },
    {
      "name": "vector_search",
      "type": "span",
      "duration_ms": 45,
      "metadata": {
        "top_k": 5,
        "similarity_threshold": 0.7
      }
    },
    {
      "name": "llm_generation",
      "type": "generation",
      "model": "llama3.2",
      "input_tokens": 850,
      "output_tokens": 320,
      "duration_ms": 3200
    }
  ]
}
```

---

## 📝 Резюме

При каждом запросе пользователя происходит следующая цепочка:

1. **Traefik** → SSL терминация и маршрутизация
2. **Open WebUI** → Аутентификация и UI
3. **Ollama** → Генерация ответа LLM
4. **Langfuse** → Логирование и трейсинг
5. **(Опционально) n8n** → RAG workflow с Supabase

Все компоненты работают в единой Docker-сети `ai-network` и общаются друг с другом по внутренним DNS-именам контейнеров.

---

**Последнее обновление: Декабрь 2024**
